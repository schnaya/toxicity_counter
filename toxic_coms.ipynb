{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85fc8cacb3af1058",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T12:39:13.240286Z",
     "start_time": "2024-10-23T12:39:07.588410Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from wordcloud import WordCloud\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "from catboost import CatBoostRegressor"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4ba0a85871b472a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T07:18:25.061824Z",
     "start_time": "2024-10-23T07:18:12.598319Z"
    }
   },
   "source": [
    "train= pd.read_csv('data/train.csv')\n",
    "validation = pd.read_csv('data/validation.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f62ab7b3e5ecbeb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T06:28:06.531534Z",
     "start_time": "2024-10-22T06:28:06.516621Z"
    }
   },
   "source": [
    "train.shape, validation.shape, test.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b60f7a81e5f5161",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T10:57:57.679001Z",
     "start_time": "2024-10-16T10:57:57.235004Z"
    }
   },
   "source": [
    "train = train.dropna(subset=['comment_text'])\n",
    "train.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d0b4cc1aa492463",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T10:42:03.875928Z",
     "start_time": "2024-10-15T10:42:03.796933Z"
    }
   },
   "source": [
    "fig = plt.figure(figsize = (10, 5))\n",
    "lang_counts = validation.groupby('lang').size().reset_index(name='count')\n",
    "plt.bar(lang_counts['lang'], lang_counts['count'], color ='magenta', \n",
    "        width = 0.4)\n",
    "\n",
    "plt.xlabel(\"Languages\")\n",
    "plt.ylabel(\"Number of comments\")\n",
    "plt.title(\"Comments on different language counts\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d8188cd4c9e4c3fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T10:42:03.971928Z",
     "start_time": "2024-10-15T10:42:03.876930Z"
    }
   },
   "source": [
    "fig = plt.figure(figsize = (10, 5))\n",
    "lang_counts = test.groupby('lang').size().reset_index(name='count')\n",
    "plt.bar(lang_counts['lang'], lang_counts['count'], color ='magenta', \n",
    "        width = 0.4)\n",
    "plt.xlabel(\"Languages\")\n",
    "plt.ylabel(\"Number of comments\")\n",
    "plt.title(\"Comments on different language counts\")\n",
    "plt.show()\n",
    "print(train.head())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2fe3388a7909f77f",
   "metadata": {},
   "source": [
    "### Exploring train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe6ab0b3498ff607",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:00:48.305369Z",
     "start_time": "2024-10-16T10:58:02.324105Z"
    }
   },
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^A-Za-z\\s\\u00C0-\\u017F\\u4e00-\\u9fff]', '', text)\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if token.lemma_ not in stop_words and not token.is_punct and not token.is_space]\n",
    "    return ' '.join(tokens)\n",
    "train['cleaned_comment_text'] = train['comment_text'].apply(lambda x: preprocess_text(str(x)))\n",
    "train[['comment_text', 'cleaned_comment_text']].head()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5337aa5bdb9a9a6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:00:54.543589Z",
     "start_time": "2024-10-16T14:00:48.306369Z"
    }
   },
   "source": [
    "all_tokens = []\n",
    "for comment in train['cleaned_comment_text']:\n",
    "    all_tokens.extend(comment.split(' '))  \n",
    "unique_tokens = set(all_tokens)\n",
    "token_dict = {token: index for index, token in enumerate(unique_tokens)}\n",
    "print(dict(list(token_dict.items())[:10]))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ef570c20520bccab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T05:34:43.610114Z",
     "start_time": "2024-10-16T05:34:15.212575Z"
    }
   },
   "source": [
    "train.to_csv('data/train.csv', index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fd3710e31aaac5e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T06:31:33.336113Z",
     "start_time": "2024-10-16T06:31:33.073603Z"
    }
   },
   "source": [
    "columns_of_interest = ['toxic', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n",
    "correlation_matrix = train[columns_of_interest].corr()\n",
    "print(correlation_matrix)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix of Toxicity Parameters')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "36d79f154c0d7e89",
   "metadata": {},
   "source": [
    "### TF-IDF + XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a17d792b41ea063c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:07:35.108980Z",
     "start_time": "2024-10-16T14:00:54.544589Z"
    }
   },
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer(vocabulary=token_dict,max_features=10000,min_df=2)\n",
    "svd = TruncatedSVD(n_components=500)  # Установите количество компонент\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(train['cleaned_comment_text'])\n",
    "tfidf_df = pd.DataFrame.sparse.from_spmatrix(tfidf_matrix, columns=vectorizer.get_feature_names_out())\n",
    "tfidf_reduced = svd.fit_transform(tfidf_matrix)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bef200f8f790b3ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:07:36.532979Z",
     "start_time": "2024-10-16T14:07:35.110980Z"
    }
   },
   "source": [
    "\n",
    "X_array = tfidf_reduced\n",
    "y_array = train['toxic'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_array, y_array, test_size=0.2, random_state=42)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2ade22d65ea0830",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T15:29:29.664280Z",
     "start_time": "2024-10-16T14:07:36.533980Z"
    }
   },
   "source": [
    "model = xgb.XGBRegressor(n_estimators=70, max_depth=25, \n",
    "                          tree_method='hist', device='cuda', \n",
    "                          random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "scores = cross_val_score(model, X_array, y_array, cv=5, scoring='neg_mean_squared_error')\n",
    "mean_mse_cv = -scores.mean() \n",
    "print(f'Mean Squared Error (Test Set): {mse}')\n",
    "print(f'R² Score (Test Set): {r2}')\n",
    "print(f'Mean Squared Error from Cross-Validation: {mean_mse_cv}')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5088bf167c5b1e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T05:25:14.529162Z",
     "start_time": "2024-10-17T05:25:13.294113Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "joblib.dump(model, 'xgb_model.pkl')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "147f8d281c6f6f3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T10:42:47.871278Z",
     "start_time": "2024-10-16T09:29:45.306429Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "model = xgb.XGBRegressor(n_estimators=100, max_depth=50, \n",
    "                          tree_method='hist', device='cuda', \n",
    "                          random_state=42)\n",
    "scores = cross_val_score(model, X_array, y_array, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "mean_mse = -scores.mean()\n",
    "print(f'Mean Squared Error from Cross-Validation: {mean_mse}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7ba1f83bd04c043e",
   "metadata": {},
   "source": [
    "Основные метрики для регрессии:\n",
    "\n",
    "Mean Squared Error (MSE): Средняя квадратическая ошибка, которая показывает, насколько сильно предсказанные значения отличаются от фактических.\n",
    "\n",
    "R² (коэффициент детерминации): Показывает, какая доля дисперсии в данных объясняется моделью. Чем ближе к 1, тем лучше модель объясняет зависимость данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff37a62f5844d1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T10:14:33.190584Z",
     "start_time": "2024-10-17T09:05:18.629158Z"
    }
   },
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "scores = cross_val_score(model, X_array, y_array, cv=5, scoring='neg_mean_squared_error')\n",
    "mean_mse_cv = -scores.mean()\n",
    "\n",
    "# Print the metrics\n",
    "print(f'Mean Squared Error (Test Set): {mse}')\n",
    "print(f'R² Score (Test Set): {r2}')\n",
    "print(f'Mean Squared Error from Cross-Validation: {mean_mse_cv}')\n",
    "\n",
    "# Plotting true vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, color='red')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('True vs Predicted Values (XGBoost)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Residuals plot\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True, color='blue', bins=30)\n",
    "plt.axvline(x=0, color='red', linestyle='--')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Residuals (XGBoost)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Feature importance plot (XGBoost)\n",
    "plt.figure(figsize=(10, 6))\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.bar(range(len(importances)), importances[indices], align='center')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance (XGBoost)')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "378c633dc722a435",
   "metadata": {},
   "source": [
    "### Doc2Vec и CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4826be95462cf208",
   "metadata": {},
   "source": [
    "\n",
    "train_tagged = [TaggedDocument(words=row.split(), tags=[str(i)]) for i, row in enumerate(train['cleaned_comment_text'])]\n",
    "\n",
    "# Обучаем модель Doc2Vec\n",
    "doc2vec_model = Doc2Vec(vector_size=100, window=2, min_count=1, workers=4, epochs=40)\n",
    "doc2vec_model.build_vocab(train_tagged)\n",
    "doc2vec_model.train(train_tagged, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "\n",
    "# Преобразуем тексты в векторы\n",
    "X_doc2vec = [doc2vec_model.infer_vector(row.split()) for row in train['cleaned_comment_text']]\n",
    "X_doc2vec = pd.DataFrame(X_doc2vec)\n",
    "\n",
    "# Обучение CatBoost\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_doc2vec, train['toxic'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Определяем модель CatBoost с использованием GPU\n",
    "catboost_model = CatBoostRegressor(iterations=1000, depth=10, learning_rate=0.1, \n",
    "                                    loss_function='RMSE', devices='0', verbose=100)\n",
    "\n",
    "# Обучаем модель\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Предсказываем и оцениваем модель\n",
    "y_pred = catboost_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error with CatBoost: {mse}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9af825daf09403d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T16:11:23.275748Z",
     "start_time": "2024-10-17T11:14:13.480545Z"
    }
   },
   "source": [
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "pipeline = joblib.load('catboost_pipeline_with_doc2vec.pkl')\n",
    "y_pred = pipeline.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "scores = cross_val_score(pipeline, X_test, y_test, cv=5, scoring='neg_mean_squared_error')\n",
    "mean_mse_cv = -scores.mean()\n",
    "\n",
    "print(f'Mean Squared Error (Test Set): {mse}')\n",
    "print(f'R² Score (Test Set): {r2}')\n",
    "print(f'Mean Squared Error from Cross-Validation: {mean_mse_cv}')\n",
    "\n",
    "# Построение графика зависимости истинных и предсказанных значений\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, color='red')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('True vs Predicted Values (CatBoost)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Построение графика остатков (ошибок)\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True, color='blue', bins=30)\n",
    "plt.axvline(x=0, color='red', linestyle='--')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Residuals (CatBoost)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Построение графика важности признаков (CatBoost)\n",
    "plt.figure(figsize=(10, 6))\n",
    "importances = pipeline.named_steps['catboost'].feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.bar(range(len(importances)), importances[indices], align='center')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance (CatBoost)')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5489862b",
   "metadata": {},
   "source": [
    "train= pd.read_csv('data/train.csv')\n",
    "validation = pd.read_csv('data/validation.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "train_sample = train.sample(n=10000, random_state=42)\n",
    "train_df = train_sample[['cleaned_comment_text']].copy()\n",
    "train_df['lang'] = 'en'\n",
    "train_df.rename(columns={'cleaned_comment_text': 'content'}, inplace=True)\n",
    "test_df = test[['content', 'lang']].copy()\n",
    "combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "combined_df = combined_df.dropna()\n",
    "combined_df.to_csv('data/combined.csv')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "95fd4973",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2e5be5c7e41df68",
   "metadata": {},
   "source": [
    "## Lang detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e860f97506b84ff",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-23T12:39:19.251083Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from classes.Doc2VecTransformer import Doc2VecTransformer\n",
    "combined_df = pd.read_csv('data/combined.csv')\n",
    "doc2vec_model = Doc2VecTransformer(device='cpu',vector_size=5000,trainings=False,epochs = 20,train_df=combined_df,model_path='models/doc2vec/doc2vec.model')\n",
    "\n",
    "vectors = doc2vec_model.transform_to_vectors(combined_df['content'])\n",
    "combined_df[\"vectors\"] = [row for row in vectors]\n",
    "unique_langs = combined_df['lang'].unique()\n",
    "lang_to_id = {lang: idx for idx, lang in enumerate(unique_langs)}\n",
    "combined_df['lang_id'] = combined_df['lang'].map(lang_to_id)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd66867fe956a7d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T07:18:36.926721Z",
     "start_time": "2024-10-23T07:18:36.773492Z"
    }
   },
   "source": [
    "fig = plt.figure(figsize = (10, 5))\n",
    "lang_counts = combined_df.groupby('lang').size().reset_index(name='count')\n",
    "plt.bar(lang_counts['lang'], lang_counts['count'], color ='magenta', \n",
    "        width = 0.4)\n",
    "\n",
    "plt.xlabel(\"Languages\")\n",
    "plt.ylabel(\"Number of comments\")\n",
    "plt.title(\"Comments on different language counts\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4906d51793dd7180",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T06:28:55.770131Z",
     "start_time": "2024-10-22T06:28:55.761131Z"
    }
   },
   "source": [
    "print(lang_counts['count'].mean)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4db0bee1ba73edcf",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc91542c8f9fc6e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T11:38:01.065638Z",
     "start_time": "2024-10-23T11:38:00.992641Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from classes.Doc2VecTransformer import Doc2VecTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class LogisticRegressionTorch(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegressionTorch, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + torch.exp(-z))\n",
    "\n",
    "X = np.array(combined_df['vectors'].tolist())\n",
    "y = np.array(combined_df['lang_id'])\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.long)  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)  # Ожидается (n_samples, n_features)\n",
    "print(y_train.shape)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X_train, X_test = X_train.to(device), X_test.to(device)\n",
    "y_train, y_test = y_train.to(device), y_test.to(device)\n",
    "input_size = X_train.shape[1]\n",
    "num_classes = len(unique_langs)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff199a723308f605",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T11:27:27.366497Z",
     "start_time": "2024-10-23T11:27:27.366497Z"
    }
   },
   "source": [
    "model = LogisticRegressionTorch(input_size, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0  \n",
    "    num_batches = 0  \n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()  \n",
    "        num_batches += 1 \n",
    "    average_loss = total_loss / num_batches\n",
    "    losses.append(average_loss)\n",
    "    if (epoch) % 10 == 0:\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(num_epochs), losses, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predicted = [] \n",
    "    all_true = []    \n",
    "\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        test_outputs = model(batch_X)\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "        \n",
    "        all_predicted.extend(predicted.cpu().numpy()) \n",
    "        all_true.extend(batch_y.cpu().numpy())       \n",
    "\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "predicted_np = np.array(all_predicted)\n",
    "y_test_np = np.array(all_true)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_np, predicted_np)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test_np), yticklabels=np.unique(y_test_np))\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55881d76",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def save_model(model, model_path):\n",
    "    # Ensure the directory exists\n",
    "    dir_name = os.path.dirname(model_path)\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Save the model's state dictionary\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "    except PermissionError:\n",
    "        raise RuntimeError(f\"Permission denied: Unable to save the model to {model_path}.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to save the model: {str(e)}\")\n",
    "\n",
    "save_model(model, 'models/lang_detect/logistic_regression_torch.pth')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dee2fa29654b613d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T07:53:04.060397Z",
     "start_time": "2024-10-23T07:35:15.745597Z"
    }
   },
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from toxic_coms_task.classes.Doc2VecTransformer import Doc2VecTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess data\n",
    "doc2vec_model = Doc2VecTransformer(model_path='models/doc2vec/doc2vec_transformer.pkl')\n",
    "combined_df = combined_df.dropna()\n",
    "vectors = doc2vec_model.transform(combined_df['content'])\n",
    "combined_df[\"vectors\"] = [row for row in vectors]\n",
    "unique_langs = combined_df['lang'].unique()\n",
    "lang_to_id = {lang: idx + 1 for idx, lang in enumerate(unique_langs)}\n",
    "combined_df['lang_id'] = combined_df['lang'].map(lang_to_id)\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_df['vectors'], combined_df['lang_id'], test_size=0.2, random_state=42)\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "scoring = make_scorer(roc_auc_score, needs_proba=True, multi_class='ovr')\n",
    "\n",
    "grid = GridSearchCV(estimator=rf, param_grid=param_grid, scoring=scoring, cv=5, verbose=1, n_jobs=-1)\n",
    "grid.fit(X_resampled, y_resampled)\n",
    "predictions = grid.best_estimator_.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "roc_auc = roc_auc_score(y_test, grid.best_estimator_.predict_proba(X_test), multi_class='ovr')\n",
    "\n",
    "print('Best Hyperparameters:', grid.best_params_)\n",
    "print('Accuracy:', accuracy)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "848584453c5b6feb",
   "metadata": {},
   "source": [
    "Try to detect in validation data(only es,it,tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "821a46392b6c82a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T10:32:36.072453Z",
     "start_time": "2024-10-23T10:32:35.216582Z"
    }
   },
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "new_tfidf_matrix = doc2vec_model.transform(validation['comment_text'])\n",
    "validation['lang_id'] = validation['lang'].map(lang_to_id)\n",
    "new_predictions = model.predict(new_tfidf_matrix)\n",
    "\n",
    "accuracy = accuracy_score(validation['lang_id'], new_predictions)\n",
    "print('Accuracy:', accuracy)\n",
    "print(classification_report(validation['lang_id'], new_predictions))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7af07aa776c48c97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T05:23:04.351507Z",
     "start_time": "2024-10-18T05:23:04.325507Z"
    }
   },
   "source": [
    "joblib.dump(model, 'language_detection.pkl')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f09793141467b42",
   "metadata": {},
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Словарь моделей для перевода с разных языков на английский\n",
    "lang_to_model = {\n",
    "    \"ru\": \"Helsinki-NLP/opus-mt-ru-en\",  # Модель для перевода с русского на английский\n",
    "    \"es\": \"Helsinki-NLP/opus-mt-es-en\",  # Модель для перевода с испанского на английский\n",
    "    \"fr\": \"Helsinki-NLP/opus-mt-fr-en\",  # Модель для перевода с французского на английский\n",
    "    # Добавьте другие языки по необходимости\n",
    "}\n",
    "\n",
    "def translate_text(text, src_lang):\n",
    "    # Определяем модель перевода по языку источника\n",
    "    model_name = lang_to_model.get(src_lang, None)\n",
    "    \n",
    "    if model_name is None:\n",
    "        raise ValueError(f\"No translation model found for language: {src_lang}\")\n",
    "    \n",
    "    # Загружаем модель и токенизатор для выбранного языка\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Токенизация текста и получение перевода\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    translated = model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "    return translated_text\n",
    "\n",
    "# Пример использования:\n",
    "text = \"Привет, как дела?\"\n",
    "src_lang = \"ru\"  # Язык источника\n",
    "translated_text = translate_text(text, src_lang)\n",
    "print(f\"Перевод: {translated_text}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5abc428c89a9800b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0ef1943091803f",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
